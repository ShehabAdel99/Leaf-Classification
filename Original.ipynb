{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nhDANOVH1SNj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras import models,layers\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras import models,layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set data\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/dataset of deep learning project/train.csv')\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(\"Training Set Info:\")\n",
    "print(train_df.info())\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"\\nFirst Few Rows of Training Set:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Display the size of the data\n",
    "print(\"\\nThe Size:\")\n",
    "print(train_df.shape)\n",
    "\n",
    "# Describe the data\n",
    "print(\"\\nThe description:\")\n",
    "print(train_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = train_df.isnull().sum()\n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = train_df.duplicated().sum()\n",
    "print(\"\\nDuplicate Entries:\")\n",
    "print(duplicates)\n",
    "\n",
    "# Remove duplicates\n",
    "train_df = train_df.drop_duplicates()\n",
    "\n",
    "# Remove missing values\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "# Separate features into margin, shape, and texture\n",
    "margin_features = train_df.iloc[:, 2:66]\n",
    "shape_features = train_df.iloc[:, 66:130]\n",
    "texture_features = train_df.iloc[:, 130:]\n",
    "\n",
    "# Correlation Analysis and display as a heatmap for margin features\n",
    "corr_margin = margin_features.corr()\n",
    "\n",
    "# Set the upper triangle to NaN\n",
    "mask_margin = np.triu(np.ones_like(corr_margin, dtype=bool))\n",
    "\n",
    "# Plotting the heatmap for margin features\n",
    "plt.figure(figsize=(50, 35))\n",
    "sns.heatmap(corr_margin, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask_margin)\n",
    "plt.title(\"Correlation Heatmap for Margin Features\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Analysis and display as a heatmap for shape features\n",
    "corr_shape = shape_features.corr()\n",
    "\n",
    "# Set the upper triangle to NaN\n",
    "mask_shape = np.triu(np.ones_like(corr_shape, dtype=bool))\n",
    "\n",
    "# Plotting the heatmap for shape features\n",
    "plt.figure(figsize=(50, 35))\n",
    "sns.heatmap(corr_shape, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask_shape)\n",
    "plt.title(\"Correlation Heatmap for Shape Features\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Analysis and display as a heatmap for texture features\n",
    "corr_texture = texture_features.corr()\n",
    "\n",
    "# Set the upper triangle to NaN\n",
    "mask_texture = np.triu(np.ones_like(corr_texture, dtype=bool))\n",
    "\n",
    "# Plotting the heatmap for texture features\n",
    "plt.figure(figsize=(50, 35))\n",
    "sns.heatmap(corr_texture, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask_texture)\n",
    "plt.title(\"Correlation Heatmap for Texture Features\")\n",
    "plt.show()\n",
    "\n",
    "# Display info after cleaning\n",
    "print(\"\\nCleaned Training Set Info:\")\n",
    "print(train_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80, 20))\n",
    "\n",
    "columns = 4\n",
    "i = 0\n",
    "\n",
    "for col in train_df.select_dtypes('object'):\n",
    "    value_counts = train_df[col].value_counts()\n",
    "    num_values = len(value_counts)\n",
    "    plt.subplot(int(num_values / columns + 1), columns, i + 1)\n",
    "    sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(col)\n",
    "    i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing images\n",
    "image_folder = '/content/drive/MyDrive/dataset of deep learning project/images'\n",
    "\n",
    "# Get a list of all image files in the folder\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(\"Number of Images:\", len(image_files))\n",
    "\n",
    "# Display some random images with a colormap\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(5):  # Change 5 to the desired number of images to display\n",
    "    random_image = random.choice(image_files)\n",
    "    image_path = os.path.join(image_folder, random_image)\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(img, cmap='gray')  # Change 'gray' to your desired colormap\n",
    "    plt.title(f\"Image: {random_image}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Clean the images\n",
    "# Check for corrupted images\n",
    "corrupted_images = []\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        img.verify()\n",
    "    except (IOError, SyntaxError):\n",
    "        corrupted_images.append(image_file)\n",
    "\n",
    "# Remove corrupted images\n",
    "for corrupted_image in corrupted_images:\n",
    "    image_files.remove(corrupted_image)\n",
    "\n",
    "# Display info after cleaning\n",
    "print(\"\\nNumber of Cleaned Images:\", len(image_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_df.species\n",
    "train = train_df.drop(['species','id'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabelEncoder  = preprocessing.LabelEncoder()\n",
    "LabelEncoder.fit(label)\n",
    "num_label = LabelEncoder.transform(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(LabelEncoder.classes_)\n",
    "max_label = num_label.max()\n",
    "min_label = num_label.min()\n",
    "print(max_label)\n",
    "print(min_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train, num_label ,test_size = 0.2,random_state = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the mean and std deviation\n",
    "\n",
    "# gets mean and std deviation\n",
    "scaler = preprocessing.StandardScaler()\n",
    "xTrain = scaler.fit_transform(x_train)\n",
    "xVal = scaler.transform(x_test)\n",
    "\n",
    "print(scaler.mean_)\n",
    "print(f\"\\n\\n{scaler.scale_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train, y_train)\n",
    "prediction = rf.predict(x_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc=SVC()\n",
    "svc.fit(x_train,y_train)\n",
    "prediction = svc.predict(x_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "prediction = logreg.predict(x_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(LabelEncoder.classes_)\n",
    "max_label = num_label.max()\n",
    "min_label = num_label.min()\n",
    "print(max_label)\n",
    "print(min_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "66rf-QtJ1fC3"
   },
   "outputs": [],
   "source": [
    "# Path to the folder containing the numbered images\n",
    "image_folder = r\"E:\\Semester 9\\Deep Learning\\Project\\dataset\\images\"\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = r\"E:\\Semester 9\\Deep Learning\\Project\\dataset\\train.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Function to load and parse images\n",
    "def load_images_and_labels(image_folder, df, target_size=(1620, 1050)):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        image_id = row['id']\n",
    "        species = row['species']\n",
    "\n",
    "        # Construct the image file path based on the ID\n",
    "        image_path = os.path.join(image_folder, f\"{image_id}.jpg\")\n",
    "\n",
    "        # Load the image using OpenCV in grayscale mode\n",
    "        #image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "        # Resize the image to the target size with replicated padding\n",
    "        height, width = image.shape\n",
    "        new_height, new_width = target_size\n",
    "\n",
    "        # Calculate padding\n",
    "        pad_height = max(0, (new_height - height) // 2)\n",
    "        pad_width = max(0, (new_width - width) // 2)\n",
    "\n",
    "        # Replicate padding\n",
    "        image = cv2.copyMakeBorder(image, pad_height, pad_height, pad_width, pad_width, cv2.BORDER_REPLICATE)\n",
    "\n",
    "        # Resize the image to the target size\n",
    "        image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "        # Convert OpenCV image to TensorFlow tensor\n",
    "        numpy_array = np.array(image)\n",
    "\n",
    "         # Append the image and label to the lists\n",
    "        data.append(numpy_array)\n",
    "        labels.append(species)\n",
    "\n",
    "    return data,labels\n",
    "\n",
    "# Load images and labels\n",
    "images,labels = load_images_and_labels(image_folder, df, target_size=(1620, 1050))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_labels = np.array(labels)\n",
    "updated_images= np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 1620, 1050)\n",
      "(990,)\n"
     ]
    }
   ],
   "source": [
    "print(updated_images.shape)\n",
    "print(updated_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updated_images= updated_images.reshape(len(updated_images),1620,1050,1)\n",
    "updated_labels= updated_labels.reshape(len(updated_labels),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 1620, 1050, 1)\n",
      "(990, 1)\n"
     ]
    }
   ],
   "source": [
    "print(updated_images.shape)\n",
    "print(updated_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "LabelEncoder  = preprocessing.LabelEncoder()\n",
    "LabelEncoder.fit(updated_labels)\n",
    "num_labels = LabelEncoder.transform(updated_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "3drgATUq1vWs"
   },
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "\n",
    "# First Convolutional layer with (3,3) matrix size, same padding refers to add additional columns or rows to align\n",
    "# with the filter and a maxpooling layers which takes maximum value in the (3,3) pixel value matrix.\n",
    "network.add(layers.Conv2D(10,(3,3), padding='same', activation='relu', input_shape = (1620,1050,1)))\n",
    "network.add(layers.MaxPool2D((3,3)))\n",
    "\n",
    "# Second convolutional layer same as above but this one produces 512 feature maps unlike 10 above.\n",
    "network.add(layers.Conv2D(512,(3,3), padding='same', activation='relu'))\n",
    "network.add(layers.MaxPool2D((3,3)))\n",
    "\n",
    "# Third convolutional layer same as above but this one produces 256 feature maps unlike 512 above.\n",
    "network.add(layers.Conv2D(256,(3,3), padding='same', activation='relu'))\n",
    "network.add(layers.MaxPool2D((3,3)))\n",
    "\n",
    "# Fourth convolutional layer same as above but this one produces 128 feature maps unlike 256 above.\n",
    "network.add(layers.Conv2D(128,(3,3), padding='same', activation='relu'))\n",
    "network.add(layers.MaxPool2D((3,3)))\n",
    "\n",
    "# Flatten layer\n",
    "network.add(layers.Flatten())\n",
    "\n",
    "# Dropout layer of 25%\n",
    "network.add(layers.Dropout(0.25))\n",
    "\n",
    "# Dense layer of 512 neurons with L2 regularization and ReLU activation function\n",
    "network.add(layers.Dense(512, kernel_regularizer = regularizers.l2(0.01), activation='relu'))\n",
    "\n",
    "# Dropout layer of 20%\n",
    "network.add(layers.Dropout(0.2))\n",
    "\n",
    "# Dense Layer of 512 neurons with L2 regularization and ReLU activation function\n",
    "network.add(layers.Dense(512, kernel_regularizer = regularizers.l2(0.01), activation='relu'))\n",
    "\n",
    "# Final Dense Layer with number of neurons same as number of classes of classification and 'softmax' as the\n",
    "# activation function\n",
    "# softmax is useful because it converts the output layer into what is essentially a probability distribution.\n",
    "network.add(layers.Dense(99, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "-HNFDfFI1wKt"
   },
   "outputs": [],
   "source": [
    "\"\"\"Compiling the network\"\"\"\n",
    "# The network has to be compiled before using it for fitting the data.\n",
    "network.compile(optimizer='adam',\n",
    "                loss=CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "PBmc4-KWLGHz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(updated_images, num_labels, test_size=0.2, random_state=5, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_one_hot = to_categorical(Y_train, num_classes = 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oNqPzNB11H2"
   },
   "outputs": [],
   "source": [
    "train_results = network.fit(x=X_train,y=Y_train,batch_size=8,      # Number of samples per gradient update\n",
    "    epochs=2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
